# src/eval.py

# ==============================================================
# Chargement des biblioth√®ques (relevant pour l'√©valuation)
# ==============================================================
import numpy as np
from sklearn.metrics import mean_squared_error, r2_score
import joblib # Pour charger le mod√®le et les donn√©es de test
import os     # Pour la gestion des chemins de fichiers

# ==============================================================
# Fonction d'√©valuation du mod√®le
# ==============================================================
def evaluate_model(data_dir="data/processed", model_path="models/model.pkl"):
    """
    Charge les donn√©es de test et le mod√®le entra√Æn√©, puis √©value la performance.
    """
    print("üîç D√©but de l'√©valuation du mod√®le...")
    
    # Chargement des donn√©es de test et du mod√®le
    try:
        X_test = joblib.load(os.path.join(data_dir, 'X_test.pkl'))
        y_test = joblib.load(os.path.join(data_dir, 'y_test.pkl'))
        model = joblib.load(model_path)
        print("‚úÖ Donn√©es de test et mod√®le charg√©s avec succ√®s.")
    except FileNotFoundError as e:
        print(f"‚ùå Erreur: Fichiers manquants. Assurez-vous que data_process.py et train.py ont √©t√© ex√©cut√©s. D√©tail: {e}")
        return

    # √âtape 7 : √âvaluation du mod√®le
    y_test_pred = model.predict(X_test)

    # Calcul des m√©triques
    rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))
    r2_test = r2_score(y_test, y_test_pred)

    # Affichage des r√©sultats
    print("\n" + "="*50)
    print("üìä R√âSULTATS DE TEST")
    print("="*50)
    print(f"RMSE Test: {rmse_test:,.2f}")
    print(f"R¬≤ Test: {r2_test:.4f}")
    print("\n‚úÖ √âvaluation termin√©e.")


# ==============================================================
# Ex√©cution du script
# ==============================================================
if __name__ == "__main__":
    evaluate_model()